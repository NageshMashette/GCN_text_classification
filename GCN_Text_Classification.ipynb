{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6e0e15",
   "metadata": {},
   "source": [
    "Here are a few common terms used in Graph Neural Networks (GNNs):\n",
    "\n",
    "Graph: A graph is a collection of nodes (also known as vertices) and edges that connect these nodes. In a GNN, the graph represents the structure of the data, and the nodes represent the individual data points.\n",
    "\n",
    "Node Embedding: A node embedding is a low-dimensional representation of a node in a graph, which captures its features and relationships to other nodes. In GNNs, the node embeddings are learned through the network and updated during training.\n",
    "\n",
    "Adjacency Matrix: An adjacency matrix is a square matrix that represents the connections between nodes in a graph. In a GNN, the adjacency matrix is used to propagate information from one node to its neighbors.\n",
    "\n",
    "Message Passing: Message passing refers to the process of exchanging information between nodes in a GNN. In each iteration of the message passing process, the information stored in the node embeddings is passed to the neighboring nodes and updated to reflect the collective information of the entire graph.\n",
    "\n",
    "Attention Mechanism: An attention mechanism is a mechanism used in GNNs to weigh the contributions of different nodes in a graph when making predictions. This helps the network focus on the most relevant parts of the graph and reduce the impact of noisy or irrelevant information.\n",
    "\n",
    "Convolutional Operation: A convolutional operation is a mathematical operation that is used in GNNs to aggregate information from the neighboring nodes in a graph. Convolutional operations are the building blocks of GNNs and are used to update the node embeddings.\n",
    "\n",
    "Graph Pooling: Graph pooling refers to the process of aggregating information from the nodes in a graph to produce a compact representation of the entire graph. In GNNs, graph pooling is used to reduce the computational complexity and capture the global structure of the graph.\n",
    "\n",
    "These are some of the main terms used in GNNs, but there are many more concepts and techniques that are used in different variants of GNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bead201",
   "metadata": {},
   "source": [
    "- steps\n",
    "\n",
    "Graph Convolutional Networks (GCNs) are used in text classification tasks to capture the relationships between words or documents in the text. Here's a step-by-step explanation of the GCN-based text classification process:\n",
    "\n",
    "Preprocessing: Prepare the text data for input into the GCN model. This includes splitting the text into individual words or documents, creating a vocabulary of unique words, and mapping each word to a unique index.\n",
    "\n",
    "Graph Construction: Create a graph representation of the text data by defining relationships between the words or documents. This can be done using an adjacency matrix, where the edges between words or documents represent their relationships.\n",
    "\n",
    "Embedding: Convert the words or documents into numerical representations called embeddings. This can be done using an embedding layer in the GCN model, which maps the words to a high-dimensional space where the relationships between words can be captured.\n",
    "\n",
    "Convolution: Perform convolutional operations on the graph to learn the relationships between words or documents. This can be done using graph convolutional layers in the GCN model, which update the embeddings of the words based on their relationships with other words in the graph.\n",
    "\n",
    "Prediction: Use the updated embeddings to make a prediction about the sentiment or topic of the text. This can be done using a fully connected layer in the GCN model, which outputs a single scalar value that represents the sentiment or topic.\n",
    "\n",
    "Training: Train the GCN model using labeled text data by minimizing the error between the predicted sentiment or topic and the ground-truth sentiment or topic.\n",
    "\n",
    "Evaluation: Evaluate the GCN model using metrics such as accuracy, precision, recall, and F1 score, to assess its performance on the text classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61b856",
   "metadata": {},
   "source": [
    "## training text classification using GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec1589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 7.1664\n",
      "Epoch 2/100, Loss: 7.7222\n",
      "Epoch 3/100, Loss: 8.2097\n",
      "Epoch 4/100, Loss: 8.6298\n",
      "Epoch 5/100, Loss: 8.9837\n",
      "Epoch 6/100, Loss: 9.2774\n",
      "Epoch 7/100, Loss: 9.5169\n",
      "Epoch 8/100, Loss: 9.7087\n",
      "Epoch 9/100, Loss: 9.8589\n",
      "Epoch 10/100, Loss: 9.9728\n",
      "Epoch 11/100, Loss: 10.0574\n",
      "Epoch 12/100, Loss: 10.1197\n",
      "Epoch 13/100, Loss: 10.1657\n",
      "Epoch 14/100, Loss: 10.1994\n",
      "Epoch 15/100, Loss: 10.2239\n",
      "Epoch 16/100, Loss: 10.2419\n",
      "Epoch 17/100, Loss: 10.2552\n",
      "Epoch 18/100, Loss: 10.2650\n",
      "Epoch 19/100, Loss: 10.2723\n",
      "Epoch 20/100, Loss: 10.2778\n",
      "Epoch 21/100, Loss: 10.2821\n",
      "Epoch 22/100, Loss: 10.2854\n",
      "Epoch 23/100, Loss: 10.2881\n",
      "Epoch 24/100, Loss: 10.2902\n",
      "Epoch 25/100, Loss: 10.2919\n",
      "Epoch 26/100, Loss: 10.2934\n",
      "Epoch 27/100, Loss: 10.2946\n",
      "Epoch 28/100, Loss: 10.2956\n",
      "Epoch 29/100, Loss: 10.2965\n",
      "Epoch 30/100, Loss: 10.2972\n",
      "Epoch 31/100, Loss: 10.2979\n",
      "Epoch 32/100, Loss: 10.2985\n",
      "Epoch 33/100, Loss: 10.2990\n",
      "Epoch 34/100, Loss: 10.2994\n",
      "Epoch 35/100, Loss: 10.2999\n",
      "Epoch 36/100, Loss: 10.3002\n",
      "Epoch 37/100, Loss: 10.3006\n",
      "Epoch 38/100, Loss: 10.3009\n",
      "Epoch 39/100, Loss: 10.3012\n",
      "Epoch 40/100, Loss: 10.3015\n",
      "Epoch 41/100, Loss: 10.3018\n",
      "Epoch 42/100, Loss: 10.3020\n",
      "Epoch 43/100, Loss: 10.3022\n",
      "Epoch 44/100, Loss: 10.3025\n",
      "Epoch 45/100, Loss: 10.3027\n",
      "Epoch 46/100, Loss: 10.3029\n",
      "Epoch 47/100, Loss: 10.3031\n",
      "Epoch 48/100, Loss: 10.3032\n",
      "Epoch 49/100, Loss: 10.3034\n",
      "Epoch 50/100, Loss: 10.3036\n",
      "Epoch 51/100, Loss: 10.3038\n",
      "Epoch 52/100, Loss: 10.3039\n",
      "Epoch 53/100, Loss: 10.3041\n",
      "Epoch 54/100, Loss: 10.3043\n",
      "Epoch 55/100, Loss: 10.3044\n",
      "Epoch 56/100, Loss: 10.3046\n",
      "Epoch 57/100, Loss: 10.3047\n",
      "Epoch 58/100, Loss: 10.3049\n",
      "Epoch 59/100, Loss: 10.3050\n",
      "Epoch 60/100, Loss: 10.3051\n",
      "Epoch 61/100, Loss: 10.3053\n",
      "Epoch 62/100, Loss: 10.3054\n",
      "Epoch 63/100, Loss: 10.3055\n",
      "Epoch 64/100, Loss: 10.3057\n",
      "Epoch 65/100, Loss: 10.3058\n",
      "Epoch 66/100, Loss: 10.3059\n",
      "Epoch 67/100, Loss: 10.3061\n",
      "Epoch 68/100, Loss: 10.3062\n",
      "Epoch 69/100, Loss: 10.3063\n",
      "Epoch 70/100, Loss: 10.3064\n",
      "Epoch 71/100, Loss: 10.3066\n",
      "Epoch 72/100, Loss: 10.3067\n",
      "Epoch 73/100, Loss: 10.3068\n",
      "Epoch 74/100, Loss: 10.3069\n",
      "Epoch 75/100, Loss: 10.3070\n",
      "Epoch 76/100, Loss: 10.3072\n",
      "Epoch 77/100, Loss: 10.3073\n",
      "Epoch 78/100, Loss: 10.3074\n",
      "Epoch 79/100, Loss: 10.3075\n",
      "Epoch 80/100, Loss: 10.3076\n",
      "Epoch 81/100, Loss: 10.3077\n",
      "Epoch 82/100, Loss: 10.3078\n",
      "Epoch 83/100, Loss: 10.3079\n",
      "Epoch 84/100, Loss: 10.3081\n",
      "Epoch 85/100, Loss: 10.3082\n",
      "Epoch 86/100, Loss: 10.3083\n",
      "Epoch 87/100, Loss: 10.3084\n",
      "Epoch 88/100, Loss: 10.3085\n",
      "Epoch 89/100, Loss: 10.3086\n",
      "Epoch 90/100, Loss: 10.3087\n",
      "Epoch 91/100, Loss: 10.3088\n",
      "Epoch 92/100, Loss: 10.3089\n",
      "Epoch 93/100, Loss: 10.3090\n",
      "Epoch 94/100, Loss: 10.3091\n",
      "Epoch 95/100, Loss: 10.3092\n",
      "Epoch 96/100, Loss: 10.3093\n",
      "Epoch 97/100, Loss: 10.3094\n",
      "Epoch 98/100, Loss: 10.3095\n",
      "Epoch 99/100, Loss: 10.3096\n",
      "Epoch 100/100, Loss: 10.3098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\n72\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the GCN layer\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.linear(x)\n",
    "        x = torch.spmm(adj, x)\n",
    "        return x\n",
    "\n",
    "# Define the sentiment analysis model\n",
    "class SentimentAnalysisGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(SentimentAnalysisGCN, self).__init__()\n",
    "        self.gcn1 = GCN(in_features, hidden_features)\n",
    "        self.gcn2 = GCN(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gcn1(x, adj))\n",
    "        x = F.log_softmax(self.gcn2(x, adj), dim=1)\n",
    "        return x\n",
    "\n",
    "# Generate the input data\n",
    "sentences = [[\"I love this movie\", 1],\n",
    "             [\"I hate this movie\", 0],\n",
    "             [\"This movie is ok\", 1]]\n",
    "             #[\"This movie is terrible\", 0]]\n",
    "\n",
    "# Pre-process the data and convert to tensors\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [sentence[0] for sentence in sentences]\n",
    "labels = np.array([sentence[1] for sentence in sentences])\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(texts).toarray()\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "adj = torch.eye(len(texts), dtype=torch.float32)\n",
    "target = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Initialize the model and move to GPU if available\n",
    "model = SentimentAnalysisGCN(x.shape[1], 100, 2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "x = x.to(device)\n",
    "adj = adj.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    output = model(x, adj)\n",
    "    loss = criterion(output, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    print(\"Epoch {}/{}, Loss: {:.4f}\".format(epoch+1, num_epochs, total_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09034912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the trained model\n",
    "torch.save(model.state_dict(), \"sentiment_analysis_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12dc41dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentAnalysisGCN(\n",
       "  (gcn1): GCN(\n",
       "    (linear): Linear(in_features=6, out_features=100, bias=True)\n",
       "  )\n",
       "  (gcn2): GCN(\n",
       "    (linear): Linear(in_features=100, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = SentimentAnalysisGCN(x.shape[1], 100, 2)\n",
    "model.load_state_dict(torch.load(\"sentiment_analysis_model.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1a7dab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 1., 0., 1.],\n",
      "        [0., 0., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1.]])\n",
      "Predictions: [1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "model = SentimentAnalysisGCN(x.shape[1], 100, 2)\n",
    "model.load_state_dict(torch.load(\"sentiment_analysis_model.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "# Define a function for performing inference on a new dataset\n",
    "def inference(new_texts):\n",
    "    # Pre-process the new sentences and convert to tensor\n",
    "    new_x = vectorizer.transform(new_texts).toarray()\n",
    "    new_x = torch.tensor(new_x, dtype=torch.float32)\n",
    "    new_x = new_x.to(device)\n",
    "    print(new_x)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(new_x, adj)\n",
    "        prediction = torch.argmax(output, dim=1).tolist()\n",
    "    return prediction\n",
    "\n",
    "# Test the inference function on a new dataset\n",
    "new_texts = [\"This movie is great\", \"I love this movie\", \"This movie is terrible\"]\n",
    "predictions = inference(new_texts)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4581c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8a270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f43c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8a91e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50096bbd",
   "metadata": {},
   "source": [
    "## loading data from .csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72246c",
   "metadata": {},
   "source": [
    "### GCN code for text classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ade6f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the data from the .csv file\n",
    "data = pd.read_csv(\"sentiment_analysis_data.csv\")\n",
    "texts = data[\"text\"].tolist()\n",
    "labels = data[\"label\"].tolist()\n",
    "\n",
    "# Pre-process the data and convert to tensors\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(texts).toarray()\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create the adjacency matrix for the graph Laplacian\n",
    "adj = torch.tensor(np.ones((x.shape[0], x.shape[0])), dtype=torch.float32)\n",
    "\n",
    "# Define the GCN model\n",
    "class SentimentAnalysisGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(torch.spmm(adj, self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = SentimentAnalysisGCN(x.shape[1], 100, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    output = model(x, adj)\n",
    "    loss = F.cross_entropy(output, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"sentiment_analysis_model.pth\")\n",
    "\n",
    "# Define a function for performing inference on a new dataset\n",
    "def inference(new_texts):\n",
    "    # Pre-process the new sentences and convert to tensor\n",
    "    new_x = vectorizer.transform(new_texts).toarray()\n",
    "    new_x = torch.tensor(new_x, dtype=torch.float32)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(new_x, adj)\n",
    "        prediction = torch.argmax(output, dim=1).tolist()\n",
    "    return prediction\n",
    "\n",
    "# Test the inference function on a new dataset\n",
    "new_texts = [\"This movie is great\", \"I love this movie\", \"This movie is terrible\"]\n",
    "predictions = inference(new_texts)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc9c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
